1.	随机变量的相关性用p，并没有描述时序特征，那么，如何描述时序的相关。（带时序的特征选择问题）
a)	序列的自相关会导致相关系数无效
b)	Pearson 系数用来描述线性。（如果是线性的话，可以和线性模型结合，P和R值的定量关系？）（有严格条件，具体？）
c)	Pearson 系数对异常值敏感
d)	（有条件）要求变量服从双变量正态分布——分别正态+联合分布正态（这条假设宽松，违反时候的系数估计也是稳健，为什么？）
2.	为什么用交叉熵损失？
a)	（熵和信息量的关系）熵是信息量的期望
b)	（交叉熵本质是KL散度）Loss是h(y, y*)= -sum_x(y(x)*log(y*(x)))。模型预测信息量在真实分布下的期望。减掉一个真实样本的熵（分布给定了，熵为常数），得到KL散度。
c)	（KL散度含义）KL散度是两个分布信息熵的差值在真实分布下的期望，本质上衡量训练后分布和真实分布的差异
d)	（KL>0）Jensen 不等式，期望内部如果是凸函数，那么E(f)>f(E)。
e)	（KL的凸性？）这个基本没人在讨论
f)	（交叉熵优势）
g)	（为什么不用KL散度？）交叉熵和极大似然有更直接的关系（具体推导？）
h)	（两种表达式-t_i *ln(q_i) & -sum_i(t_i*ln(y_i)+(1-t_i)*ln(1-y_i）)是代表不同的输出层，softmax和 sigmoid。Softmax是把全部的神经元输出作为一个概率分布。Sig是把单个的神经元的输出概率作为一个二项分布。

3.	为什么用对数损失？
a)	对数损失在二分类下等价于交叉熵损失

4.	为什么用softmax 
a)	可以生成一个概率分布，做多分类问题
b)	Softmax在更新梯度的时候计算快，求导更新的结果是目标上的输出概率-1（交叉熵Loss）。
c)	配合对数似然使用也可以避免梯度消失
d)	（和sigmoid对比）sigmoid是把一个输出映射到一个区间，做二分类。Softmax把一个向量做归一化，做多分类问题。
e)	特征对概率的影响是乘性的

5.	Softmax分类器
a)	（和LR分类器的用途区别）LR是S分类器在n=2的特例。若多类别相独立，用softmax，不独立，用多个LR分类器。
b)	本质上是一个单层的神经网络

6.	LR
a)	连续变量要做标准化（？）
b)	（样本分布敏感）不适用非平衡数据集（待验证）
c)	高维特征不影响（？）
d)	评价用ROC（？）TPR FPR
e)	非线性的情况下一定要做feature mapping

7.	FM 因子分解机/ FFM ——特征组合

8.	SVD
