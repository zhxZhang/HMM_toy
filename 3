#coding:utf-8
import numpy as np
from pandas import Series, DataFrame
import pandas as pd
# import jieba package for word segmentation
import jieba
import re
# from sklearn.cross_validation import train_test_split
import sys

reload(sys)
sys.setdefaultencoding("utf-8")

"""
word segmentation for raw data
"""

class Data_Process(object):
    def feature_extract(self, file_in, outputdir):
        """
        extract the key features
        file_in contains 2 columns.[label, title]
        """
        self.input_file = file_in
        self.output_dir = outputdir

        # read csv format data
        df = pd.read_csv(self.input_file, header=0, dtype=str)

        self.features = df
        self.features = self.features.fillna('')


    def extract_samples(self, num):

        # sampling from rows
        self.featuers = self.features.sample(n=num)



    def word_segmentation(self):
        """
        word segmentation for the text
        """

        pattern1 = re.compile("/")
        # remove all other symbols
        pattern2 = re.compile("[\".:%<>&#+]")

        # merge all the features into features_totalï¼ˆjzhä¿®æ”¹ä¸ºåªå‰©titleæ ‡ç­¾ï¼?
        # self.features['features_total'] = self.features['keyword']+self.features['title']+self.features['info']+self.features['info2']+self.features['productDe']+self.features['description']
        self.features['features_total'] = self.features['title']

        # convert to string
        self.features['features_total'] = self.features['features_total'].map(str)

        # word segment via jieba
        fenci_func = lambda x: jieba.cut(x, cut_all=False)

        # substitute symbol "/" with ' '
        sub_symbol = lambda x: pattern1.sub(' ', x)
        # remove all other symbols
        remove_symbol = lambda x: pattern2.sub('', x)
        # convert to lower letter
        lower_letter = lambda x: x.lower()

        self.features['features_total'] = self.features['features_total'].map(fenci_func).map(" ".join).map(
            sub_symbol).map(remove_symbol).map(lower_letter)  # .map(join_func)

        # remove the noise of label
        # pattern = re.compile(r"([0-9]{4})")
        pattern = re.compile(r"([0-9]{4,5})")       #jzh(ä¿®æ”¹æ­£åˆ™åŒ¹é…)
        remove_noise_of_label = lambda x: pattern.match(x).group()  # only match the beginning
        self.features['label'] = self.features['label'].map(str).map(remove_noise_of_label)

        # remove two classes: 1411,1408. The two classes overlap with other classes
        keep_cls_index = ((self.features['label'] != '1411') & (self.features['label'] != '10104')& (self.features['label'] != '10105')& (self.features['label'] != '10084')& (self.features['label'] != '10072')& (self.features['label'] != '10052')& (self.features['label'] != '10047')& (self.features['label'] != '10033')& (self.features['label'] != '10032')& (self.features['label'] != '10031')& (self.features['label'] != '10023')& (self.features['label'] != '10017')& (self.features['label'] != '10013'))
        self.features = self.features[keep_cls_index]
        # label_array = [10001,10002,.....]

    def label2int(self, int2label_path):

        with open(int2label_path, 'r') as w:
            sum137 = w.readlines()
            ids = {}
            for o in sum137:
                o = o.replace('\n', '').split(':')
                ids[o[1]] = o[0]

        label_array = self.features['label'].unique()
        ids = {k.replace('\r',''): v for k, v in ids.iteritems()}      

        exist_labels = list(ids.keys())
        # print exist_labels

        index_func = lambda x: ids[x] if x in exist_labels else 999
        self.features['label'] = self.features['label'].map(index_func).map(str)
        # print self.features['label']
        self.features['features_total'] = self.features['features_total'].map(str)


    def gen_test_data(self):
        """
        generate test data set
        """

        # _, X_test, _, y_test = train_test_split(self.features['features_total'], self.features['label'],test_size=1.0, random_state=42)

        X_test, y_test = self.features['features_total'], self.features['label']

        test_doc = y_test + '\t' + X_test

        # remove short samples
        rmv_short_func = lambda x: x if (len(x) > 7) else np.NaN
        test_doc = test_doc.map(rmv_short_func)
        test_doc = test_doc[test_doc.notnull()]

        # output training data size
        print("test_doc count %d" % test_doc.count())
        print('test label\n')
        print('test unique label num:',len(y_test.unique()))
        print(y_test.unique())

        # output test data to txt file
        tmp_dir = self.output_dir + '/test.txt'
        test_doc.to_csv(tmp_dir, index=False)


if __name__ == '__main__':
    # file='data_amazon.csv'
    # outputdir='./data_cleaned'

    file_in = sys.argv[1]
    outputdir = sys.argv[2]
    num_extract_samples = sys.argv[3]
    int2label_path = sys.argv[4]
    
    print 'num_extract_samples :', num_extract_samples

    process_object = Data_Process()
    # # extract the key features    #æŒ‘é€‰ä¸»è¦çš„å±žæ€?
    process_object.feature_extract(file_in, outputdir)
    # # sampling
    if int(num_extract_samples) > 0:
        print 'excute extract func'
        process_object.extract_samples(num_extract_samples)
    # # word segmentation for the text    æ–‡æœ¬åˆ†è¯jiaba
    process_object.word_segmentation()

    process_object.label2int(int2label_path)

    # # divide data into training and test set   å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•é›†
    process_object.gen_test_data()






import os
import sys
import datahelper.datloader as helper
import datahelper.dataset_utils as data_helper
import tensorflow as tf

# config file path
config_fname = sys.argv[1]

# load configure file
config_dict= helper.load_config(config_fname)
resource_folder = config_dict['resource_folder'] + '/'

print "loading dictonary..."
vocabulary = helper.json_load_vocab(resource_folder + config_dict['d2c_dict_fname'])

# load data
print "loading train&test data..."
_, _, x_test, y_test, _ =  helper.load_data(train_path="", test_path=config_dict['test_data'],\
                                            sequence_length=config_dict['doc_maxlen'], \
                                            vocabulary = vocabulary, padding = True)

# print y_test
print ("single x_test item's length: %d. " % (len(x_test[0])))
print ("number of x_test : %d." % len(x_test))



# the number of classes
#num_category = max(y_test) + 1
num_category = 121

print "converting test data..."
data_helper.convert_tokens_and_labels(x_test, y_test, config_dict['tf_test_data'], num_category)

# get the number of classes of test data
nc_test = data_helper.get_tfrecord_classes_counts(config_dict['tf_test_data'])
print ("zzx nc_test  %d \t"%(nc_test))
print "Tfrecord Saved."

import os
import re

message = os.popen('nvidia-smi').readlines()
# print type(str(message))

pattern = r"(\d*)MiB"
match = re.findall(pattern, str(message))
match = [int(MiB) for MiB in match ]
# print match

num_gpus = 15
least_mib = 999999
best_gpu = 0

for gpu in range(num_gpus):
	if match[2*gpu] < least_mib:
		least_mib = match[2*gpu]
		best_gpu = gpu

print best_gpu


config=configure/config_zx_test.txt      #config file path           #1
num_epochs=200                     #max epoch for pretraining
embedding_size=384                  #word embedding dimension
batch_size=1
dropout_keep_prob=0.8
word_dropout_keep_prob=0.8
cost_type=base
early_stop_patience=10
lstm_hidden_units=256
softmax_hidden_units=128
pretrain_word2vec_path=/home/xue.mao/D2C_train_55/wiki_wordvec/wiki.zh.bin
#pretrain_word2vec_path=/home/hlluo/xydu/mykr/pretrained_wordvec/SogouNews-wordvec256.bin
#pretrain_word2vec_path=None

l2_reg_lambda=0.001
start_learning_rate=0.002
lr_decay_method='exp'  #not use learning rate decay method

#ul_batch_size
#test_batch_size
#vat_epsilon

gpu_id=$(python 7.3-find_free_gpu.py)              #2
echo $gpu
devices=/gpu:$gpu_id
model_method=cnn
model_restore_path=model_zx_121_deprecated/base_$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret
model_path=models/base_$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret              #3
logs_path=logt/base_$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret.log            #4

CUDA_VISIBLE_DEVICES=$gpu_id python -u val.py --config $config --device $devices --lstm_hidden_units $lstm_hidden_units --softmax_hidden_units $softmax_hidden_units --l2_reg_lambda $l2_reg_lambda --num_epochs $num_epochs --model_method $model_method --model_restore_path $model_restore_path --model_path $model_path --cost_type $cost_type --embedding_size $embedding_size --batch_size $batch_size --word_dropout_keep_prob $word_dropout_keep_prob --dropout_keep_prob $dropout_keep_prob --pretrain_word2vec_path $pretrain_word2vec_path --early_stop_patience $early_stop_patience --start_learning_rate $start_learning_rate --lr_decay_method $lr_decay_method > $logs_path



num_extract_samples=-1

csv_file_path="/home/xue.mao/D2C_train_21/propreces_data/debug_data_set.csv"
segword_output_path="/home/xue.mao/D2C_train_21/data/zx"
int2label_path="/home/xue.mao/D2C_train_21/int2label.txt"

config="/home/xue.mao/D2C_train_21/configure/config_zx_test.txt"

echo "get input."
python -c "import get_input; print get_input.get_input()"


echo "7.1-preprocess_test_data Begin."
python 7.1-preprocess_test_data.py $csv_file_path $segword_output_path $num_extract_samples $int2label_path

echo "7.2-make_tf_test_data Begin"
python 7.2-make_tf_test_data.py $config

echo "model pred begin."
source ./7.4-model_pred.sh
echo "pred result have been saved."

echo "read and print result"
result=$(python -c 'import get_input; print get_input.get_output()')
echo $result


config=configure/config_zx_121_train.txt      #config file path
num_epochs=17                    #max epoch for pretraining
embedding_size=384                  #word embedding dimension
batch_size=64
dropout_keep_prob=0.8
word_dropout_keep_prob=0.8
cost_type=base
early_stop_patience=10
lstm_hidden_units=256
softmax_hidden_units=128
pretrain_word2vec_path=/home/xue.mao/D2C_train_55/wiki_wordvec/wiki.zh.bin
#pretrain_word2vec_path=/home/hlluo/xydu/mykr/pretrained_wordvec/SogouNews-wordvec256.bin
#pretrain_word2vec_path=None

l2_reg_lambda=0.001
start_learning_rate=0.002
lr_decay_method='exp'  #not use learning rate decay method

#ul_batch_size
#test_batch_size
#vat_epsilon

gpu_id=$(python 7.3-find_free_gpu.py)
devices=/gpu:$gpu_id
model_method=cnn
model_restore_path=None
model_path=model_zx_121/base_$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret
logs_path=log_zx_121/base_$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret.log

CUDA_VISIBLE_DEVICES=$gpu_id python -u train_shopping2C.py --config $config --device $devices --lstm_hidden_units $lstm_hidden_units --softmax_hidden_units $softmax_hidden_units --l2_reg_lambda $l2_reg_lambda --num_epochs $num_epochs --model_method $model_method --model_restore_path $model_restore_path --model_path $model_path --cost_type $cost_type --embedding_size $embedding_size --batch_size $batch_size --word_dropout_keep_prob $word_dropout_keep_prob --dropout_keep_prob $dropout_keep_prob --pretrain_word2vec_path $pretrain_word2vec_path --early_stop_patience $early_stop_patience --start_learning_rate $start_learning_rate --lr_decay_method $lr_decay_method > $logs_path &



# config file path
config=configure/config_zx_dis_debug.txt
# max epoch for pre-training
num_epochs=200
# word embedding dimension
embedding_size=256
# the number of samples in one batch
batch_size=64
# 1-dropout_rate
dropout_keep_prob=0.8
# 1-dropout_rate of word embedding
word_dropout_keep_prob=0.8
# not using adversarial training or virtual adversarial training
cost_type=base
# the condition to early stop
early_stop_patience=10
#pretrain_word2vec_path=/home/xydu/project/NLP_data/GoogleNews-vectors300.bin
pretrain_word2vec_path=/home/xue.mao/D2C_train_21/wiki_wordvec/SogouNews-wordvec256.bin
#pretrain_word2vec_path=None

# the number of LSTM hidden units
lstm_hidden_units=128
# the number of units of softmax layer
softmax_hidden_units=128

# the coefficient the L2 regular term
l2_reg_lambda=0.001
# the initial learning rate
start_learning_rate=0.0002
# not use learning rate decay method
lr_decay_method=None
# the number of replicas to aggregate in distributed training
replicas_to_aggregate=2
#ul_batch_size
#test_batch_size
#vat_epsilon

# choose whether to train CNN or Bi-LSTM
model_method=cnn
# the path to restore model
model_restore_path=None
# the path to save model
model_path=model_zx_121/dis_base-$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret
# the path to save log
logs_path=log_zx_121/dis_base-$model_method-${lstm_hidden_units}x${softmax_hidden_units}-zh_pret.log

# parameter server
CUDA_VISIBLE_DEVICES='' nohup python -u train_distributed_shopping2C.py \
--job_name=ps \
--task_id=0 \
--ps_hosts=localhost:2555 \
--worker_hosts=localhost:2666,localhost:2777 2>&1 

# worker
CUDA_VISIBLE_DEVICES=1 nohup python -u train_distributed_shopping2C.py \
--job_name=worker \
--task_id=0 \
--ps_hosts=localhost:2555 \
--worker_hosts=localhost:2666,localhost:2777 \
--config $config \
--num_epochs $num_epochs \
--model_path $model_path \
--model_restore_path $model_restore_path \
--model_method $model_method \
--cost_type $cost_type \
--embedding_size $embedding_size \
--batch_size $batch_size \
--lstm_hidden_units $lstm_hidden_units \
--softmax_hidden_units $softmax_hidden_units \
--dropout_keep_prob $dropout_keep_prob \
--word_dropout_keep_prob $word_dropout_keep_prob \
--l2_reg_lambda $l2_reg_lambda \
--pretrain_word2vec_path $pretrain_word2vec_path \
--early_stop_patience $early_stop_patience \
--start_learning_rate $start_learning_rate \
--lr_decay_method $lr_decay_method \
--replicas_to_aggregate $replicas_to_aggregate \
> $logs_path 2>&1 

# worker
CUDA_VISIBLE_DEVICES=2 nohup python -u train_distributed_shopping2C.py \
--job_name=worker \
--task_id=1 \
--ps_hosts=localhost:2555 \
--worker_hosts=localhost:2666,localhost:2777 \
--config $config \
--num_epochs $num_epochs \
--model_restore_path $model_restore_path \
--model_method $model_method \
--cost_type $cost_type \
--embedding_size $embedding_size \
--batch_size $batch_size \
--lstm_hidden_units $lstm_hidden_units \
--softmax_hidden_units $softmax_hidden_units \
--dropout_keep_prob $dropout_keep_prob \
--word_dropout_keep_prob $word_dropout_keep_prob \
--l2_reg_lambda $l2_reg_lambda \
--pretrain_word2vec_path $pretrain_word2vec_path \
--early_stop_patience $early_stop_patience \
--start_learning_rate $start_learning_rate \
--lr_decay_method $lr_decay_method \
--replicas_to_aggregate $replicas_to_aggregate 2>&1 
