
树模型对于稀疏特征表现不好，尽量不做很大的One hot
树模型适合高召回率的变量？？

如果分类变量数目很多，可以考虑先进行聚类再做one-hot？怎么聚？
决策树很适合用分类变量

决策树 GBDT Xgboost怎么对连续值做处理 怎么对缺失值做处理 原理还是一样的吗？


树模型
1.不做缺失值，不做one-hot，直接输入GBDT(LightGBM）
	对分类数据做one-hot
2.DT
	sklearn dt
		string可以用hash，也可以做one hot
			pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)[source]
			原因——sklearn dt算法默认是连续数值型输入，1.hash会导致出现数值有序增加噪声，
												   2.sklearn只能转换成one-hot，因为不能处理categorical/string variables.

3.不做特征工程和处理，RF


分类	
 分类数据one-hot处理
3.SVM
4.LR

ks, auc, psi
分类变量之间的距离 汉明距离 hamming distance


缺失值
	缺失之间的相关，直接把缺失全部标成1，非缺失全部标成0。两个二分类之间做卡方检验
		如果卡方没过，在大样本情况下怎么做进一步的检验？
		这只能检验一个的相关，如果依赖于多个变量的怎么做？
	1-
		聚类填充
		knn
			注意做正则化，基于距离
			不适合高维数据
			time consuming 全数据集搜索 kd树
			originally 用于连续数据
			对于混合数据集，？

	10-20
		分类的做one-hot处理
		连续
    
    
    import tensorflow as tf
import numpy as np
import layers as L
import losses as costs
import initializations as tf_init

class DeepModel(object):
    def __init__(self, 
                 input_x, 
                 input_y, 
                 input_ul_x, 
                 params, 
                 dropout_keep_prob = 1., 
                 word_dropout_keep_prob = 1., 
                 init_w2v = None, 
                 seed = None,
                 reuse_lstm=None,
                 is_training=False):

        if seed is None:
            self.rng = np.random.RandomState(seed)
        else:
            self.rng = np.random.RandomState(self.params.init_weights_seed)

        self.params = params
        sequence_length = params.doc_maxlen
        embedding_size = params.embedding_size
        vocab_size = params.vocab_len
        num_classes = params.num_classes
         
        self.is_training = is_training
        self.dropout_keep_prob = dropout_keep_prob
        self.word_dropout_keep_prob = word_dropout_keep_prob

        self.reuse_lstm = reuse_lstm
        self.lstm_hidden_units = params.lstm_hidden_units
        self.softmax_hidden_units = params.softmax_hidden_units
 
        # word embeding
        with tf.variable_scope("embedding"):
            if init_w2v is not None:
                initializer = tf.constant(init_w2v)
                W = tf.get_variable(initializer=initializer, name="W")
            else:
                #initializer = tf.contrib.layers.variance_scaling_initializer(seed=self.rng.randint(123456))
                initializer = tf.random_uniform_initializer(-1.0, 1.0)
                W = tf.get_variable(initializer=initializer, shape=[vocab_size, embedding_size], name="W")

            input_e = L.word_dropout(tf.nn.embedding_lookup(W, input_x), word_dropout_keep_prob, seed=self.rng.randint(123456))
            if input_ul_x is not None:
                input_ul_e = L.word_dropout(tf.nn.embedding_lookup(W, input_ul_x), word_dropout_keep_prob, seed=self.rng.randint(123456))

        # base network
        if params.model_method == 'cnn':
            self.forward_train = self.cnn_forward
        elif params.model_method == 'bilstm':
            self.forward_train = self.bilstm_forward
    
        # CalculateMean cross-entropy loss
        logits = self.forward_train(input_e, add_weight_decay=self.is_training) 
        nll_loss = L.categorical_crossentropy(logits, input_y)
        
        additional_loss = 0
        # is_training = False will not call to build addtional model in validation and test phase.
        if self.is_training:
            self.reuse_lstm = True
            scope = tf.get_variable_scope()
            scope.reuse_variables()
            
            if self.params.cost_type.upper() == "BASE":
                additional_loss = 0
            elif self.params.cost_type.upper() == "AT":
                additional_loss = costs.adversarial_loss(input_e, input_y, self.forward_train,
                                                                 self.params.vat_epsilon)
            elif self.params.cost_type.upper() == "VAT":
                additional_loss = costs.virtual_adversarial_loss(input_ul_e, self.forward_train,
                                                                 self.params.vat_epsilon)
            elif self.params.cost_type.upper() == "VATENT":
                additional_loss = costs.virtual_adversarial_loss(input_ul_e, self.forward_train,
                                                                 self.params.vat_epsilon)
                ent_loss = L.entropy_y_x(self.forward_train(input_ul_e))
                additional_loss +=  ent_loss

        with tf.variable_scope("loss"):
            if self.is_training:
                l2_loss = tf.add_n(tf.get_collection('weight_decays'), name='wd_loss')
            else:
                l2_loss = 0
 
            nll_loss = tf.identity(nll_loss, name = 'nll_loss')
            self.loss = tf.identity(nll_loss + params.vat_lambda*additional_loss + params.l2_reg_lambda * l2_loss, name='total_loss')

        with tf.variable_scope("output"):
            self.scores = tf.nn.softmax(logits, name='scores')
            self.predictions = tf.argmax(self.scores, 1, name="predictions")

        # Accuracy
        with tf.variable_scope("accuracy"):
            self.correct_predictions = tf.equal(self.predictions, tf.argmax(input_y, 1), name="correct_predictions")
            self.accuracy = tf.reduce_mean(tf.cast(self.correct_predictions, "float"), name="accuracy")

    # def cnn forward network
    def cnn_forward(self, x, is_training=True, update_batch_stats=False, add_weight_decay=False):
        # Note that this random state is used to keep the dropout out settings the same in AT and VAT training.
        dropout_rng = np.random.RandomState(1234)
        if len(x.shape) == 3:
            x = tf.expand_dims(x, -1)

        filter_sizes = [3,4,5]
        num_filters = 150
        feature_size = self.softmax_hidden_units
        sequence_length, embedding_size = int(x.shape[1]), int(x.shape[2])

        # Create a convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.variable_scope("conv-maxpool-%s" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, embedding_size, 1, num_filters]

#                initializer = tf.contrib.layers.variance_scaling_initializer(seed=self.rng.randint(123456))
                initializer = tf.truncated_normal_initializer(stddev=0.1, seed=dropout_rng.randint(123456)) 
                W = tf.get_variable(initializer=initializer, shape=filter_shape, name="W")
                b = tf.get_variable(initializer=tf.constant_initializer(0.), shape=[num_filters], name="b")
#                W = tf_init.get_init(init = 'uniform', shape=filter_shape, name = "W")
#                b = tf_init.get_init(init = 'zero', shape=[num_filters], name = "b")

                conv = tf.nn.conv2d(x,  W,
                       strides=[1, 1, 1, 1],
                       padding="VALID",
                       name="conv")

                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")

                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                         h,
                         ksize=[1, sequence_length - filter_size + 1, 1, 1],
                         strides=[1, 1, 1, 1],
                         padding='VALID',
                         name="pool")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        h_pool = tf.concat(pooled_outputs, 3)
        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])

        # Final (unnormalized) scores
        with tf.variable_scope("cnn-output"):
            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob, seed=dropout_rng.randint(123456))

#            initializer = tf.contrib.layers.variance_scaling_initializer(seed=self.rng.randint(123456))
            initializer = tf.truncated_normal_initializer(stddev=0.1, seed=self.rng.randint(123456)) 
            W0 = tf.get_variable(initializer=initializer, shape=[num_filters_total, feature_size], name="W0")
            b0 = tf.get_variable(initializer=tf.constant_initializer(0.), shape=[feature_size], name="b0")
  
            h_drop = tf.nn.dropout(tf.nn.relu(tf.nn.xw_plus_b(h_drop, W0, b0)), self.dropout_keep_prob, seed=dropout_rng.randint(123456))

#            initializer = tf.contrib.layers.variance_scaling_initializer(seed=self.rng.randint(123456)) 
            initializer = tf.truncated_normal_initializer(stddev=0.1, seed=self.rng.randint(123456)) 
            W = tf.get_variable(initializer=initializer, shape=[feature_size, self.params.num_classes], name="W")
            b = tf.get_variable(initializer=tf.constant_initializer(0.), shape=[self.params.num_classes], name="b")

            if add_weight_decay:
                tf.add_to_collection('weight_decays', tf.nn.l2_loss(W0))
                tf.add_to_collection('weight_decays', tf.nn.l2_loss(W)) 
        
            logits = tf.nn.xw_plus_b(h_drop, W, b, name="logits")

        return logits

    # def bi-LSTM forward network
    def bilstm_forward(self, x, is_training=True, update_batch_stats=False, add_weight_decay=False):
        
        # Note that this random state is used to keep the dropout out settings the same in AT and VAT training.
        dropout_rng = np.random.RandomState(1234)
        # x shape [batch_size, sequence_length, embedding_size]
        sequence_length = x.get_shape().as_list()[1]
        
        # convert x's shape to list as time_steps * [batch_size, embedding_size]
        x = tf.unstack(x, sequence_length, 1)
        
        lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_hidden_units, reuse=self.reuse_lstm, forget_bias=0.0)
        lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_hidden_units, reuse=self.reuse_lstm, forget_bias=0.0)
        
        lstm_out, state_fw, state_bw = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell, 
                                                                               lstm_bw_cell, 
                                                                               x,
                                                                               dtype=tf.float32)
        
        # The first time step of bw output.
        first_bw_output = tf.split(lstm_out[0], 2, axis=1)[1]
    
        # The last time step of fw output.
        last_fw_output = tf.split(lstm_out[sequence_length-1], 2, axis=1)[0]
    
        # Concate the two lstm outputs.
        output = tf.concat([first_bw_output, last_fw_output], axis=1)
        
        output = tf.nn.dropout(output, self.dropout_keep_prob, seed=dropout_rng.randint(123456))
        
        softmax_input = output
        input_shape = softmax_input.get_shape().as_list()[1]
        
        # Hidden layer
        layer_weights = tf.get_variable('softmax_hidden_w', 
                                        [input_shape, self.softmax_hidden_units],
                                        initializer=tf.contrib.layers.xavier_initializer(seed=self.rng.randint(123456)))
                                                         
        layer_biases = tf.get_variable('softmax_hidden_b', 
                                       [self.softmax_hidden_units], 
                                       initializer=tf.zeros_initializer())
        
        
        layer = tf.nn.xw_plus_b(softmax_input, layer_weights, layer_biases)
        layer = tf.nn.relu(layer)
        
        layer = tf.nn.dropout(layer, self.dropout_keep_prob, seed=dropout_rng.randint(123456))
    
        # Output layer
        output_weights = tf.get_variable('softmax_output_w', 
                                         [self.softmax_hidden_units, self.params.num_classes],
                                         initializer=tf.contrib.layers.xavier_initializer(seed=self.rng.randint(123456)))
        output_biases = tf.get_variable('softmax_output_b', [self.params.num_classes], 
                                        initializer=tf.zeros_initializer())
        
        if add_weight_decay:
            tf.add_to_collection('weight_decays', tf.nn.l2_loss(layer_weights)) 
            tf.add_to_collection('weight_decays', tf.nn.l2_loss(output_weights))
            
        with tf.variable_scope("bilstm-output"):
            logits = tf.nn.xw_plus_b(layer, output_weights, output_biases, name="logits")
    
        # Output is raw logits without softmax
        return logits
        
        
        
        
        

