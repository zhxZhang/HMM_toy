
高维类ID特征
	可以考虑直接做，但是前提是数据量足够大
	和其他特征组合
		在ID上累计历史统计量，转ID为CTR(时序才能累加?)
	神经网络embedding
		通过训练投影到64/128维的向量
		怎么训练，如果没有监督
			word2vec
	TBS算法-Catboost
	

L1正则的线性模型也有选择特征的作用

相关性图谱！

LR特征不需要同时算好？


需要做层次化的特征
	eg.地理位置相关特征结合用户年龄段特征。
	让特征具有层次化结构，从粗粒度到细粒度结合做特征。

连续变量离散化算法

embedding方法
	FM
	FMM
	Graph Embedding
	流形学习


KL变换-类似PCA
	LLE

基于距离的注意特征的归一化


import tensorflow as tf
import sys, os
import initializations as tf_init
import numpy as np
import ast
_EPSILON = 10e-8
_FLOATX = "float32"

FLAGS = tf.app.flags.FLAGS
# moving average decay factor
tf.app.flags.DEFINE_float('bn_stats_decay_factor', 0.99,
                          "moving average decay factor for stats on batch normalization")


def lrelu(x, a=0.1):
    """
    Define lrelu activation function
    """
    if a < 1e-16:
        return tf.nn.relu(x)
    else:
        return tf.maximum(x, a * x)


def bn(x, dim, is_training=True, update_batch_stats=True, collections=None, name="bn"):
    """
    batch normalization
    """
    params_shape = (dim,)
    n = tf.to_float(tf.reduce_prod(tf.shape(x)[:-1]))
    axis = list(range(int(tf.shape(x).get_shape().as_list()[0]) - 1))
    mean = tf.reduce_mean(x, axis)
    var = tf.reduce_mean(tf.pow(x - mean, 2.0), axis)
    avg_mean = tf.get_variable(
        name=name + "_mean",
        shape=params_shape,
        initializer=tf.constant_initializer(0.0),
        collections=collections,
        trainable=False
    )

    avg_var = tf.get_variable(
        name=name + "_var",
        shape=params_shape,
        initializer=tf.constant_initializer(1.0),
        collections=collections,
        trainable=False
    )
    # the variable that controls the batch normalization
    gamma = tf.get_variable(
        name=name + "_gamma",
        shape=params_shape,
        initializer=tf.constant_initializer(1.0),
        collections=collections
    )

    beta = tf.get_variable(
        name=name + "_beta",
        shape=params_shape,
        initializer=tf.constant_initializer(0.0),
        collections=collections,
    )

    if is_training:
        avg_mean_assign_op = tf.no_op()
        avg_var_assign_op = tf.no_op()
        if update_batch_stats:
            avg_mean_assign_op = tf.assign(
                avg_mean,
                FLAGS.bn_stats_decay_factor * avg_mean + (1 - FLAGS.bn_stats_decay_factor) * mean)
            avg_var_assign_op = tf.assign(
                avg_var,
                FLAGS.bn_stats_decay_factor * avg_var + (n / (n - 1))
                * (1 - FLAGS.bn_stats_decay_factor) * var)

        with tf.control_dependencies([avg_mean_assign_op, avg_var_assign_op]):
            z = (x - mean) / tf.sqrt(1e-6 + var)
    else:
        z = (x - avg_mean) / tf.sqrt(1e-6 + avg_var)

    return gamma * z + beta


def fc(x, dim_in, dim_out, seed=None, name='fc'):
    """
    Fully collected layer
    """
    num_units_in = dim_in
    num_units_out = dim_out
    weights_initializer = tf.contrib.layers.variance_scaling_initializer(seed=seed)
    # define the weights w of fully collected layer
    weights = tf.get_variable(name + '_W',
                            shape=[num_units_in, num_units_out],
                            initializer=weights_initializer)
    # define the biases b of fully collected layer
    biases = tf.get_variable(name + '_b',
                             shape=[num_units_out],
                             initializer=tf.constant_initializer(0.0))
    # wx+b
    x = tf.nn.xw_plus_b(x, weights, biases)
    return x


def conv(x, ksize, stride, f_in, f_out, padding='SAME', use_bias=False, seed=None, name='conv'):
    """
    Convolution layer
    """
    # filter size
    shape = [ksize, ksize, f_in, f_out]
    initializer = tf.contrib.layers.variance_scaling_initializer(seed=seed)
    weights = tf.get_variable(name + '_W',
                            shape=shape,
                            dtype='float',
                            initializer=initializer)
    # convolution operation
    x = tf.nn.conv2d(x, weights, [1, stride, stride, 1], padding=padding)

    # whether to add bias
    if use_bias:
        bias = tf.get_variable(name + '_b',
                               shape=[f_out],
                               dtype='float',
                               initializer=tf.zeros_initializer)
        return tf.nn.bias_add(x, bias)
    else:
        return x


def conv1d(x, ksize, stride, f_in, f_out, padding='SAME', use_bias=False, seed=None, name='conv'):
    """
    Convolution along one dimension
    """
    shape = [ksize, x.shape[2], f_in, f_out]
    initializer = tf.contrib.layers.variance_scaling_initializer(seed=seed)
    weights = tf.get_variable(name + '_W',
                            shape=shape,
                            dtype='float',
                            initializer=initializer)
    # convolution operation
    x = tf.nn.conv2d(x, weights, [1, stride, stride, 1], padding=padding)

    # whether to add bias
    if use_bias:
        bias = tf.get_variable(name + '_b',
                               shape=[f_out],
                               dtype='float',
                               initializer=tf.zeros_initializer)
        return tf.nn.bias_add(x, bias)
    else:
        return x


def avg_pool(x, ksize=2, stride=2):
    """
    average pooling layer
    """
    return tf.nn.avg_pool(x,
                          ksize=[1, ksize, ksize, 1],
                          strides=[1, stride, stride, 1],
                          padding='SAME')


def max_pool(x, ksize=2, stride=2):
    """
    max pooling layer
    """
    return tf.nn.max_pool(x,
                          ksize=[1, ksize, ksize, 1],
                          strides=[1, stride, stride, 1],
                          padding='SAME')


def word_dropout(x, dropout_keep_prob, seed = None):
    """
    word dropout layer by keeping dropout_keep_prob
    """
    rng = np.random.RandomState(1234)
    sequence_length = int(x.shape[1])
    # dropout_keep_prob is the probability to keep
    if dropout_keep_prob < 1.:
        return tf.nn.dropout(x, dropout_keep_prob, noise_shape=[1, sequence_length, 1], seed=rng.randint(123456))
    return x


def ce_loss(logit, y):
    """
    cross entropy loss
    """
    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))


def accuracy(logit, y):
    """
    Compute the accuracy of prediction
    """
    # prediction result
    pred = tf.argmax(logit, 1)
    # ground-truth
    true = tf.argmax(y, 1)
    return tf.reduce_mean(tf.to_float(tf.equal(pred, true)))


def logsoftmax(x):
    """
    Compute the logsoftmax
    """
    xdev = x - tf.reduce_max(x, 1, keep_dims=True)
    lsm = xdev - tf.log(tf.reduce_sum(tf.exp(xdev), 1, keep_dims=True))
    return lsm
  

def kl_divergence_with_logit(q_logit, p_logit, include_ent_term=False):
    """
    Compute the KL-divergence with logit
    """
    # logit is the value before feeding into softmax
    q = tf.nn.softmax(q_logit)
    qlogp = tf.reduce_mean(tf.reduce_sum(q * logsoftmax(p_logit), 1))
    # whether to include entropy term
    if(include_ent_term):
        qlogq = tf.reduce_mean(tf.reduce_sum(q * logsoftmax(q_logit), 1))
        return qlogq - qlogp
    else:
        return -qlogp


def kl_divergence(q_y, p_y, main_obj_type = 'CE', include_ent_term=False):
    """
     Compute the KL-divergence according to whether it is cross entropy or quadratic entropy
    """
    # cross entropy
    if(main_obj_type=='CE'):
        p_y = tf.clip_by_value(p_y, tf.cast(_EPSILON, dtype=_FLOATX),
                                    tf.cast(1. - _EPSILON, dtype=_FLOATX))
        # whether to include entropy term
        if(include_ent_term):
            q_y = tf.clip_by_value(q_y, tf.cast(_EPSILON, dtype=_FLOATX),
                                        tf.cast(1. - _EPSILON, dtype=_FLOATX))
            return tf.reduce_mean(tf.reduce_sum(q_y * (tf.log(q_y) - tf.log(p_y)), 1))
        else:
            return - tf.reduce_mean(tf.reduce_sum(q_y * tf.log(p_y), 1))
    # quadratic entropy
    elif(main_obj_type=='QE'):
        return tf.reduce_mean(tf.reduce_sum((p_y-q_y)**2, 1))
    else:
        raise NotImplementedError()


def entropy_y_x(logit):
    """
     Define entropy based on logit
    """
    p = tf.nn.softmax(logit)
    return -tf.reduce_mean(tf.reduce_sum(p * logsoftmax(logit), 1))


def categorical_crossentropy(logits, labels):
    """
     Define categorical cross entropy based on logits and labels
    """
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
    cross_entropy_mean = tf.reduce_mean(cross_entropy)
    return cross_entropy_mean


def _variable_on_cpu(name, shape, initializer):
    """
     Define variable on cpu
    """
    with tf.device('/cpu:0'):
        var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)
    return var    
